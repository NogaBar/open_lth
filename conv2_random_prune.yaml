default_hparams: cifar_conv2
batch_size: 60
gpu: '7'
levels: 0-25
lr: 0.0002
model_name: cifar_conv2
optimizer_name: adam
replicate: 5
subcommand: lottery_branch
branch_name: train_init
#strategy: layerwise_conv
#trials: 1000
#property: features_erank
#conv_layers: true
algorithm: mutual_coherence
steps: 5000
dataset_name: cifar10
training_steps: 40ep
pruning_fraction: 0.2
pruning_layers_to_ignore: fc.weight
pruning_strategy: sparse_global
pruning_fraction_last_fc: 0.1
pruning_conv: 0.1
seed: 259
apex_fp16: false
batchnorm_frozen: false
batchnorm_init: uniform
blur_factor: null
data_order_seed: null
display_output_location: false
do_not_augment: true
evaluate_only_at_end: false
gamma: null
milestone_steps: null
model_init: kaiming_normal
momentum: 0.0
nesterov_momentum: 0.0
num_workers: 0
others_frozen: false
others_frozen_exceptions: null
output_frozen: false
platform: local
pretrain: false
quiet: false
random_labels_fraction: null
rewinding_steps: null
subsample_fraction: null
transformation_seed: null
unsupervised_labels: null
warmup_steps: null
weight_decay: null
