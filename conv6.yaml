default_hparams: cifar_conv6
batch_size: 60
gpu: '7'
levels: 15-25
lr: 0.0003
model_name: cifar_conv6
optimizer_name: adam
replicate: 5
subcommand: lottery_branch
branch_name: train_init
#property: features_erank
#trials: 1000
#lth_path: /home/noga/open_lth_data/lottery_fcd2078307525571decefce0d451fcc2/replicate_5/
#lth_data: random_color
#conv_layers: true
algorithm: erank_norm
steps: 1000
init_lr: 0.1
weight_reg_coeff: 0.000001
features_norm_coeff: 0.001
seed: 512
dataset_name: cifar10
training_steps: 40ep
pruning_fraction: 0.2
pruning_layers_to_ignore: fc.weight
pruning_strategy: sparse_global
pruning_fraction_last_fc: 0.1
pruning_conv: 0.15
apex_fp16: false
batchnorm_frozen: false
batchnorm_init: uniform
blur_factor: null
data_order_seed: null
display_output_location: false
do_not_augment: true
evaluate_only_at_end: false
gamma: null
milestone_steps: null
model_init: kaiming_normal
momentum: 0.0
nesterov_momentum: 0.0
num_workers: 3
others_frozen: false
others_frozen_exceptions: null
output_frozen: false
platform: local
pretrain: false
quiet: false
random_labels_fraction: null
rewinding_steps: null
subsample_fraction: null
transformation_seed: null
unsupervised_labels: null
warmup_steps: null
weight_decay: null
