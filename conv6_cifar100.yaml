default_hparams: cifar100_conv6
batch_size: 60
gpu: '7'
levels: 10-25
lr: 0.0003
model_name: cifar100_conv6
optimizer_name: adam
replicate: 5
subcommand: lottery_branch
branch_name: cross_domain
#property: features_erank
#trials: 1000
lth_path: /home/noga/open_lth_data/lottery_9ae2c18b35685fc44452965ff04242fc/replicate_5/
lth_data: cifar10
#conv_layers: true
#seed: 259
dataset_name: cifar100
training_steps: 40ep
pruning_fraction: 0.2
pruning_layers_to_ignore: fc.weight
pruning_strategy: sparse_global
pruning_fraction_last_fc: 0.1
pruning_conv: 0.15
#outputs: 100
apex_fp16: false
batchnorm_frozen: false
batchnorm_init: uniform
blur_factor: null
data_order_seed: null
display_output_location: false
do_not_augment: true
evaluate_only_at_end: false
gamma: null
milestone_steps: null
model_init: kaiming_normal
momentum: 0.0
nesterov_momentum: 0.0
num_workers: 4
others_frozen: false
others_frozen_exceptions: null
output_frozen: false
platform: local
pretrain: false
quiet: false
random_labels_fraction: null
rewinding_steps: null
subsample_fraction: null
transformation_seed: null
unsupervised_labels: null
warmup_steps: null
weight_decay: null
